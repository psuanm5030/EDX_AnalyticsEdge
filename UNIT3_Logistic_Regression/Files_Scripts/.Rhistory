?step
step(model_clim2)
step(model_clim1)
model_clim3 = step(model_clim1)
summary(model_clim3)
predictTest = predict(model_clim3, newdata = test)
predictTest #shows you predictions based upon the model for each observation / data point in the new df
summary(predictTest) #shows you predictions based upon the model for each observation / data point in the new df
predictTest = predict(model_clim3, newdata = test)
summary(predictTest) #shows you predictions based upon the model for each observation / data point in the new df
predictTest = predict(model_clim3, newdata = test)
summary(predictTest)
?predict
SSE = sum((predictTest - test$Temp)^2)
SST = sum((mean(train$Temp) - test$Temp)^2)
R2 = 1 - (SSE / SST)
R2 #0.8127142
pisaTrain = read.csv("pisa2009train.csv")
pisaTest = read.csv("pisa2009test.csv")
tapply(pisaTrain$readingScore,pisaTrain$male,mean)
summary(pisaTrain)
pisaTrain = na.omit(pisaTrain)
pisaTest = na.omit(pisaTest)
pisaTrain$grade
source('~/.active-rstudio-document', echo=TRUE)
lmScore = lm(readingScore ~ . , data = pisaTrain) # see the shorthand -> "~."
summary(lmScore)
SSE = sum((lmScore - pisaTrain$readingScore)^2)
SSE = sum((lmScore - pisaTrain$readingScore)^2)
SSE = sum(lmScore$residuals^2)
RMSE = sqrt(SSE / nrow(pisaTrain))
RMSE #196.3723
sqrt(mean(lmScore$residuals^2))
SSE
predTest = predict(lmScore, newdata = pisaTest)
summary(predTest)
SSE = sum((predTest - pisaTest$readingScore)^2)
SSE
SST = sum((mean(pisaTrain$readingScore) - pisaTest$readingScore)^2)
R2 = 1 - (SSE / SST)
R2 #0.8127142
RMSE = sqrt(SSE / nrow(pisaTest))
RMSE #196.3723 --- little higher than before - making an average error of 196 pts
summary(lmScore)
?mean
mean(pisaTrain$readingScore)
SST
mean(pisaTrain$readingScore) #517.9629
SST = sum((mean(pisaTrain$readingScore) - pisaTest$readingScore)^2)
SST
baseline = mean(pisaTrain$readingScore) #517.9629
SST = sum(baseline - pisaTest$readingScore)^2)
SST = sum((baseline - pisaTest$readingScore)^2)
SST #7802354
R2 = 1 - (SSE / SST)
R2 #0.8127142
FluTrain = read.csv("FluTrain.csv")
summary(FluTrain)
table(FluTrain$ILI,FluTrain$Week)
table(FluTrain$Week,FluTrain$ILI)
FluTrain
FluTrain[with(FluTrain,order(ILI)),]
FluTrain[with(FluTrain,order(Queries)),] #This orders the DF by ILI column
hist(FluTrain$ILI)
plot(log(FluTrain$ILI),FluTrain$Queries)
plot(FluTrain$Queries,log(FluTrain$ILI))
model = lm(ILI ~ Queries, data = FluTrain)
summary(model)
model = lm(log(ILI) ~ Queries, data = FluTrain)
summary(model)
Correlation = cor(FluTrain$Queries, log(FluTrain$ILI))
Correlation
Correlation^2 #= 0.7090201
FluTest = read.csv("FluTest.csv")
summary(FluTest)
PredTest1 = exp(predict(FluTrend1, newdata=FluTest))
FluTrend1 = lm(log(ILI) ~ Queries, data = FluTrain)
PredTest1 = exp(predict(FluTrend1, newdata=FluTest))
FluTest$Week
?which
PredTest1
which(FluTest$Week == "2012-03-11")
which(FluTest$Week == "2012-03-11 - 2012-03-17")
PredTest1[11]
Estimated_ILI = PredTest1[11] #2.187378
Observed_ILI = FluTest[11]
FluTest
Observed_ILI = FluTest$Week[11]
Observed_ILI
Observed_ILI = FluTest$ILI[11]
Observed_ILI
rel_error = (Observed_ILI - Estimated_ILI) / Observed_ILI
rel_error
SSE = sum(PredTest1$residuals^2)
SSE = sum(FluTrend1$residuals^2)
SSE
RMSE = sqrt(SSE / nrow(pisaTrain))
RMSE
RMSE = sqrt(SSE / nrow(FluTrain))
RMSE #0.1241895
RMSE = sqrt(SSE / nrow(FluTest))
RMSE #0.2988034
SSE = sum((PredTest1-FluTest$ILI)^2)
SSE
# root mean squared error - is going to be the square root of the
# sum of squared errors divided by n, which is the number of rows
# in our test data set.
RMSE = sqrt(SSE / nrow(FluTest))
RMSE #0.8461588
install.packages("zoo")
library(zoo)
ILILag2 = lag(zoo(FluTrain$ILI), -2, na.pad=TRUE)
FluTrain$ILILag2 = coredata(ILILag2)
summary(FluTrain$ILILag)
summary(FluTrain$ILILag2)
summary(FluTrain$ILILag2)
plot(log(FluTrain$ILI),log(FluTrain(ILILag2)))
plot(log(FluTrain$ILI),log(FluTrain$ILILag2))
plot(log(FluTrain$ILILag2), log(FluTrain$ILI))
FluTrend2 = lm(log(ILI)~Queries + ILILag2,data = FluTrain)
summary(FluTrend2)
FluTrend2 = lm(log(ILI)~Queries + log(ILILag2),data = FluTrain)
summary(FluTrend2) #0.8505
FluTrend1 = lm(log(ILI) ~ Queries, data = FluTrain)
summary(model) #R2 = 0.709
FluTrend2 = lm(log(ILI)~Queries + log(ILILag2),data = FluTrain)
summary(FluTrend2) #0.9063
ILILag2 = lag(zoo(FluTest$ILI), -2, na.pad=TRUE)
FluTest$ILILag2 = coredata(ILILag2)
summary(FluTest$ILILag2)
rows = nrow(FluTrain)
rows
FluTest$ILILag2[1] = FluTrain$ILI[rows-2]
FluTest$ILILag2[2] = FluTrain$ILI[rows-1]
FluTest[1]
FluTest$ILI[1]
FluTest$ILI[1]
FluTrain$ILI[rows-2]
FluTest$ILI[2]
FluTrain$ILI[rows-1]
FluTrain$ILI[rows-2]
FluTest$ILILag2[1]
FluTrain$ILI[rows-1]
FluTest$ILILag2[2] = FluTrain$ILI[rows-1]
FluTest$ILILag2[2]
FluTest$ILILag2[1]
rows
rows - 2
FluTrain$ILI[rows-1]
FluTest$ILILag2[1] = FluTrain$ILI[rows-1]
FluTest$ILILag2[1]
FluTrain$ILI[rows]
FluTest$ILILag2[2] = FluTrain$ILI[rows]
FluTest$ILILag2[2]
PredTest2 = exp(predict(FluTrend2, newdata=FluTest))
# What is the test-set RMSE of the FluTrend2 model?
SSE = sum((PredTest2-FluTest$ILI)^2)
SSE
RMSE = sqrt(SSE / nrow(FluTest))
RMSE #0.7490645
eAll = read.csv("elantra.csv")
eTrain = subset(eAll,Year<2013)
eTest = subset(eAll,Year>2012)
summary(eTrain)
summary(eTest)
str(eTrain)
str(eTest)
eModel = lm(ElantraSales~Unemployment + CPI_all + CPI_energy + Queries, data = eTrain)
summary(eModel)
eModel2 = lm(ElantraSales~Month + Unemployment + CPI_all + CPI_energy + Queries, data = eTrain)
summary(eModel2) #R2 -
?as.factor
eTrain$MonthF = as.factor(eTrain$Month)
eModel2 = lm(ElantraSales~ MonthF + Unemployment + CPI_all + CPI_energy + Queries, data = eTrain)
summary(eModel2) #R2 -
eTest$MonthF = as.factor(eTest$Month)
cor(eTrain)
cor(train)
cor(eTrain)
cor(eTest)
cor(eAll)
cor(ElantraTrain[c("Unemployment","Month","Queries","CPI_energy","CPI_all")])
cor(eTrain[c("Unemployment","Month","Queries","CPI_energy","CPI_all")])
summary(eModel2) #R2 - 0.8193
eModel3 = lm(ElantraSales~ MonthF + Unemployment + CPI_all + CPI_energy, data = eTrain)
summary(eModel3) #R2 -
predTest1 = predict(eModel3, newdata = eTest)
SSE = sum(predTest1$residuals^2)
SSE
SSE = sum((predTest1 - eTest$ElantraSales)^2)
SSE
mean(eTest$ElantraSales)
mean(eTrain$ElantraSales)
summary(predTest1)
SST = sum((mean(eTrain$ElantraSales) - eTest$ElantraSales)^2)
SST
R2 = 1 - (SSE / SST)
R2
RMSE = sqrt(SSE / nrow(eTest))
RMSE #0.2942029
max(abs(PredictTest - ElantraTest$ElantraSales))
max(abs(predTest1 - eTest$ElantraSales))
predTest1
which.max(abs(PredictTest - ElantraTest$ElantraSales))
which.max(abs(predTest1 - eTest$ElantraSales))
eTest[5]
predTest1 = eTest$ElantraSales
predTest1 = predict(eModel3, newdata = eTest)
predTest1 - eTest$ElantraSales
which.max(abs(predTest1 - eTest$ElantraSales)) #Returns the ROW Number
eTest[5]
eTest
state = read.csv("statedata.csv")
# Convert to a data frame
data(state)
summary(state)
statedata = cbind(data.frame(state.x77), state.abb, state.area, state.center,  state.division, state.name, state.region)
plot(statedata$y, statedata$x)
plot(statedata$x, statedata$y)
tapply(statedata$HS.Grad,state$region,mean)
tapply(statedata$HS.Grad,statedata$region,mean)
tapply(statedata$HS.Grad,statedata$region,mean())
?tapply
tapply(statedata$HS.Grad,statedata$state.region,mean)
boxplot(statedata$Murder ~ statedata$state.region)
outlier = subset(statedata, state.region == "Northeast")
outlier
outlier = subset(statedata, state.region == "Northeast" & Murder > 10)
outlier
lexp = lm(Life.Exp ~ Population + Income + Illiteracy + Murder + HS.Grad + Frost + Area, data = statedate)
lexp = lm(Life.Exp ~ Population + Income + Illiteracy + Murder + HS.Grad + Frost + Area, data = statedata)
summary(lexp)
plot(statedata$Income, statedata$Life.Exp)
summary(lexp)
lexp2(Life.Exp ~ Population + Illiteracy + Murder + HS.Grad + Frost + Area, data = statedata)
summary(lexp2)
lexp2 = lm(Life.Exp ~ Population + Illiteracy + Murder + HS.Grad + Frost + Area, data = statedata)
summary(lexp2)
summary(lexp)
summary(lexp2)
summary(lexp3) #
lexp3 = lm(Life.Exp ~ Population + Murder + HS.Grad + Frost + Area, data = statedata)
summary(lexp3) #
lexp4 = lm(Life.Exp ~ Population + Murder + HS.Grad + Frost, data = statedata)
summary(lexp4) #0.736 / 0.706
summary(lexp) #0.7362 / 0.6922
summary(lexp4) #0.736 / 0.7126 (BEST Adjusted R2)
predict(lexp4)
sort(predict(lexp4))
sort(statedata$Life.Exp)
sort(statedata$Life.Exp & statedata$state.name)
which.min(statedata$Life.Exp)
statedata$state.name[40]
sort(predict(lexp4)) # Which has the lowest predicted life expectancy
which.max(statedata$Life.Exp) #Which has the lowest life expectancy - actual
statedata$state.name[11] #Find the state name by row from previous command
which.min(lexp4$residuals)
which.max(lexp4$residuals)
sort(abs(lexp4$residuals))
which.min(abs(lexp4$residuals)
which.min(abs(lexp4$residuals))
which.max(abs(lexp4$residuals))
sort(abs(statedata$Life.Exp - predict(lexp4)))
exp(-1)
1-0.3678794
e = 1+0.3678794
1/e
e = exp(-0.3678794)
1/(1+e)
e=exp(1)
1/(1+e)
quality = read.csv("quality.csv")
setwd("/users/miller/desktop/r_edx/UNIT3/Files_Scripts")
quality = read.csv("quality.csv")
str(quality)
table(quality$PoorCare)
98/131
install.packages("caTools")
library(caTools)
set.seed(88)
split = sample.split("quality$PoorCare", SplitRatio = 0.75)
split
quality = read.csv("quality.csv")
str(quality)
#Take a look at the quality of care (1 = poor care)
table(quality$PoorCare)
library(caTools)
set.seed(88)
split = sample.split(quality$PoorCare, SplitRatio = 0.75)
split
qualityTrain = subset(quality,split == TRUE)
qualityTest = subset(quality,split == FALSE)
nrow(qualityTrain)
nrow(qualityTest)
QualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data = qualityTrain)
QualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data = qualityTrain, family = binomial)
summary(QualityLog)
predictTrain = predict(QualityLog, type="response")
predictTrain = predict(QualityLog, type="response")
summary(predictTrain)
tapply(predictTrain, qualityTrain$PoorCare,mean)
QualityLog2 = glm(PoorCare ~ StartedOnCombination + ProviderCount, data = qualityTrain, family = binomial)
summary(QualityLog2)
table(qualityTrain$PoorCare, predictTrain>0.5)
70/74
table(qualityTrain$PoorCare, predictTrain>0.5)
table(qualityTrain$PoorCare, predictTrain>0.7)
73/74
table(qualityTrain$PoorCare, predictTrain>0.2)
20/25
15/25
table(qualityTrain$PoorCare, predictTrain>0.5)
table(qualityTrain$PoorCare, predictTrain>0.7)
table(qualityTrain$PoorCare, predictTrain>0.2)
install.packages("ROCR")
library(ROCR)
ROCRpred = prediction(predictTrain,qualityTrain$PoorCare)
ROCRperf = (ROCRpred, "tpr","fpr")
ROCRperf = (ROCRpred, "tpr","fpr")
ROCRpred
ROCRperf = performance(ROCRpred,'tpr','fpr')
plot(ROCRperf)
plot(ROCRperf,colorize = TRUE)
plot(ROCRperf,colorize = TRUE,print.cutoff.at=seq(0,1,0.1))
plot(ROCRperf,colorize = TRUE,print.cutoff.at=seq(0,1,0.1), text.adj=c(-0.2,1.7))
plot(ROCRperf,colorize = TRUE,print.cutoffs.at=seq(0,1,0.1), text.adj=c(-0.2,1.7))
plot(ROCRperf,colorize = TRUE,print.cutoffs.at=seq(0,1,0.1), text.adj=c(-0.2,1.7))
plot(ROCRperf,colorize = TRUE,print.cutoffs.at=seq(0,1,0.1), text.adj=c(-0.2,4.7))
plot(ROCRperf,colorize = TRUE,print.cutoffs.at=seq(0,1,0.1), text.adj=c(-0.2,1.7))
predictTest = predict(QualityLog, type="response", newdata=qualityTest)
# You can compute the test set AUC by running the following two commands in R:
ROCRpredTest = prediction(predictTest, qualityTest$PoorCare)
auc = as.numeric(performance(ROCRpredTest, "auc")@y.values)
auc
summary(ROCRpredTest)
summary(predictTest)
summary(QualityLog)
framingham = read.csv("framingham.csv")
str(framingham)
library(caTools)
train = subset(framingham, split = TRUE)
test = subset(framingham, split = FALSE)
split = sample.split(framingham$TenYearCHD, SplitRatio = 0.65)
train = subset(framingham, split = TRUE)
test = subset(framingham, split = FALSE)
framinghamLog = glm(TenYearCHD ~ ., data = train, family = binomial)
summary(framinghamLog)
predictTest = predict(framinghamLog, type = "response", newdata = test)
table(test$TenYearCHD, predictTest > 0.5)
(1069+11)/(1069+6+187+11)
split = sample.split(framingham$TenYearCHD, SplitRatio = 0.65)
train = subset(framingham, split == TRUE)
test = subset(framingham, split == FALSE)
framinghamLog = glm(TenYearCHD ~ ., data = train, family = binomial)
summary(framinghamLog)
predictTest = predict(framinghamLog, type = "response", newdata = test)
table(test$TenYearCHD, predictTest > 0.5)
(1069+11)/(1069+6+187+11)
str(framingham)
set.seed(1000)
split = sample.split(framingham$TenYearCHD, SplitRatio = 0.65)
train = subset(framingham, split == TRUE)
test = subset(framingham, split == FALSE)
framinghamLog = glm(TenYearCHD ~ ., data = train, family = binomial)
summary(framinghamLog)
predictTest = predict(framinghamLog, type = "response", newdata = test)
# T Value of .5
table(test$TenYearCHD, predictTest > 0.5)
(1069+11)/(1069+6+187+11)
(1069+6)/(1069+6+187+11)
library(ROCR)
ROCRpred = prediction(predictTest,test$TenYearCHD)
as.numeric(performance(ROCRpred,"auc")@y.values)
pisaTrain = read.csv("pisa2009train.csv")
pisaTest = read.csv("pisa2009test.csv")
tapply(pisaTrain$readingScore,pisaTrain$male,mean) # 483.5325 male / 512.9406 female
setwd("/users/miller/desktop/r_edx/UNIT2/Files_Scripts")
pisaTrain = read.csv("pisa2009train.csv")
pisaTest = read.csv("pisa2009test.csv")
tapply(pisaTrain$readingScore,pisaTrain$male,mean) # 483.5325 male / 512.9406 female
summary(pisaTrain)
summary(pisaTrain)
#Removing missing data
pisaTrain = na.omit(pisaTrain)
pisaTest = na.omit(pisaTest)
# UNORDERED FACTORS IN REGRESSION MODELS
summary(pisaTrain$raceeth)
pisaTrain$raceeth = relevel(pisaTrain$raceeth, "White")
pisaTest$raceeth = relevel(pisaTest$raceeth, "White")
lmScore = lm(readingScore ~ . , data = pisaTrain) # see the shorthand -> "~."
summary(lmScore)
predTest = predict(lmScore, newdata = pisaTest)
summary(predTest)
lmScore = lm(readingScore ~ . , data = pisaTrain) # see the shorthand -> "~."
summary(lmScore)
11/198
setwd("/users/miller/desktop/r_edx/UNIT3/Files_Scripts")
1069/1075
polling = read.csv("PollingData.csv")
str(polling)
table(polling$Year)
summary(polling)
install.packages("mice")
library(mice)
simple = polling[c("Rasmussen","SurveyUSA","PropR","DiffCount")]
summary(simple)
set.seed(144)
imputed = complete(mice(simple))
summary(imputed)
polling$Rasmussen = imputed$Rasmussen
polling$SurveyUSA = imputed$SurveyUSA
summary(polling)
Train = subset(polling, Year == 2004 | Year = 2008)
Test = subset(polling, Year == 2012)
Train = subset(polling, Year == 2004 | Year == 2008)
Test = subset(polling, Year == 2012)
table(Train$Republican)
sign(20)
sign(-20)
sign(0)
table(sign(Train$Rasmussen))
table(Train$Republican, sign(Train$Rasmussen))
cor(Train)
str(Train)
cor(Train[c("Rasmussen","SurveyUSA","PropR","DiffCount","Republican")])
mod1 = glm(Republican ~ PropR, data = Train, family = binomial)
summary(mod1)
pred1 = predict(mod1, type = "response") # NO Newdata - just preds on training
table(Train$Republican, pred1>=0.5)
mod2 = glm(Republican ~ SurveyUSA + DiffCount, data = Train, family = binomial)
summary(mod1) #AIC - 19.772
mod2 = glm(Republican ~ SurveyUSA + DiffCount, data = Train, family = "binomial")
summary(mod2) #AIC - 19.772
mod2 = glm(Republican ~ SurveyUSA + DiffCount, data = Train, family = binomial)
summary(mod2) #AIC - 18.439 vs 19.772 for mod1
pred2 = predict(mod2, type="response")
mod2 = glm(Republican ~ SurveyUSA + DiffCount, data = Train, family = binomial)
summary(mod2) #AIC - 18.439 vs 19.772 for mod1
pred2 = predict(mod2, type="response")
table(Train$Republican, pred2>=0.5) # 4 mistakes - same as smart baseline
summary(mod2)
table(Test$Republican,sign(Test$Rasmussen))
TestPrediction = predict(mod2, newdata = Test, type=response)
TestPrediction = predict(mod2, newdata = Test, type="response")
table(Test$Republican,TestPrediction>0.5)
table(Test$Republican,TestPrediction>=0.5)
subset(Test, TestPrediction >=0.5 & Republican == 0)
songs = read.csv("songs.csv")
str(songs)
table(songs$year)
sort(table(songs$artistname))
which(songs$artistname == "Michael Jackson")
count(which(songs$artistname == "Michael Jackson"))
which(sort(table(songs$artistname)) = "Michael Jackson")
which(sort(table(songs$artistname)) == "Michael Jackson")
count(which(songs$artistname == "Michael Jackson"))
cnt(which(songs$artistname == "Michael Jackson"))
df = which(songs$artistname == "Michael Jackson")
df
cnt(df)
sum(df)
length(df)
nrow(df)
str(df)
MichaelJackson = subset(songs, artistname == "Michael Jackson")
str(MichaelJackson)
MichaelJacksonTOP = subset(MichaelJackson, Top10 == TRUE)
MichaelJacksonTOP
MichaelJacksonTOP$songtitle
length(MichaelJacksonTOP$songtitle)
MichaelJacksonTOP$songtitle
MichaelJackson[c(“songtitle”, “Top10”)]
MichaelJackson[c(“songtitle”, “Top10”)]
MichaelJackson[c('songtitle', 'Top10')]
summary(songs)
summary(songs$timesignature)
sort(songs$timesignature)
table(songs$timesignature)
which.max(songs$tempo)
songs[6206]
songs$songtitle[6206]
SongsTrain = subset(songs, year <= 2009)
SongsTest = subset(songs, year <= 2010)
str(SongsTrain)
str(SongsTest)
SongsTest = subset(songs, year == 2010)
str(SongsTrain)
str(SongsTest)
nonvars = c("year", "songtitle", "artistname", "songID", "artistID")
SongsTrain = SongsTrain[ , !(names(SongsTrain) %in% nonvars) ]
SongsTest = SongsTest[ , !(names(SongsTest) %in% nonvars) ]
SongsLog1 = glm(Top10 ~ ., data=SongsTrain, family=binomial)
summary(SongsLog1)
cor(SongsTrain$loudness, SongsTrain$energy)
SongsLog2 = glm(Top10 ~ . - loudness, data=SongsTrain, family=binomial)
summary(SongsLog2)
str(SongsTrain)
SongsLog3 = glm(Top10 ~ . - loudness - energy, data=SongsTrain, family=binomial)
summary(SongsLog3)
SongsLog3 = glm(Top10 ~ . - energy, data=SongsTrain, family=binomial)
summary(SongsLog3)
pred3 = predict(SongsLog3, newdata = SongsTest, type="response")
table(SongsTrain$Top10, pred3>0.45)
table(SongsTest$Top10, pred3>0.45)
(309+19)/(314+59)
mean(SongsTest$Top10)
table(SongsTest$Top10)
314/(314+59)
table(SongsTest$Top10, pred3>0.45)
(309+19) #0.8793566
19/59
309/314
