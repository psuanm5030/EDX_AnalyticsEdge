ntrain$Question <- grepl("?",ntrain$Question) == TRUE
ntrain$Question <- grepl("?",ntrain$Headline)
table(ntrain$Question)
ntrain$Question <- grepl("*?",ntrain$Headline)
table(ntrain$Question)
ntrain$Question <- grep("?",ntrain$Headline)
table(ntrain$Question)
ntrain$Question <- grepl("*?*",ntrain$Headline)
table(ntrain$Question)
ntrain$Question <- grepl("*?",ntrain$Headline)
table(ntrain$Question)
ntrain$Question <- grepl("\?*",ntrain$Headline)
ntrain$Question <- grepl("\?",ntrain$Headline)
?grepl
ntrain$Question <- grepl("[:puntct:]",ntrain$Headline)
table(ntrain$Question)
?regexp
ntrain$Question <- grepl("\\?",ntrain$Headline)
table(ntrain$Question)
ntrain$Question <- grepl("\\?",ntrain$Headline)
ntest$Question <- grepl("\\?",ntrain$Headline)
ntest$Question <- grepl("\\?",ntest$Headline)
ntrain = read.csv("NYTimesBlogTrain.csv", stringsAsFactors=FALSE)
ntest = read.csv("NYTimesBlogTest.csv", stringsAsFactors=FALSE)
ntrain$NewsDesk = as.factor(ntrain$NewsDesk)
ntest$NewsDesk = as.factor(ntest$NewsDesk)
ntrain$SectionName = as.factor(ntrain$SectionName)
ntest$SectionName = as.factor(ntest$SectionName)
levels(ntest$SectionName) <- levels(ntrain$SectionName)
levels(ntest$NewsDesk) <- levels(ntrain$NewsDesk)
ntrain$PubDate = strptime(ntrain$PubDate, "%Y-%m-%d %H:%M:%S")
ntest$PubDate = strptime(ntest$PubDate, "%Y-%m-%d %H:%M:%S")
# Setup new Variable - Weekday
ntrain$Weekday = ntrain$PubDate$wday
ntrain$WKday <- ntrain$Weekday == 0 | ntrain$Weekday == 6
ntest$Weekday = ntest$PubDate$wday
ntest$WKday <- ntest$Weekday == 0 | ntest$Weekday == 6
str(ntrain)
# Setup New Variable - Is there a Question Mark??
ntrain$Question <- grepl("\\?",ntrain$Headline)
ntest$Question <- grepl("\\?",ntest$Headline)
# Setup New Variable - Hour
ntrain$Hour = ntrain$PubDate$hour
ntest$Hour = ntest$PubDate$hour
# New Variable for Prime Reading Hours
# Three levels: Midnight to 7am; 7am to 6pm, 6pm to midnight
ntrain$HourCat = cut(ntrain$Hour, c(0,6,11,15,18,22,24))
table(ntrain$HourCat)
ntest$HourCat = cut(ntest$Hour, c(0,6,11,15,18,22,24))
table(ntest$HourCat)
CorpusHeadline = Corpus(VectorSource(c(ntrain$Headline, ntest$Headline)))
CorpusHeadline = tm_map(CorpusHeadline, tolower)
# Remember this extra line is needed after running the tolower step:
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
# Convert our corpus to a DocumentTermMatrix, remove sparse terms, and turn it into a data frame.
dtm = DocumentTermMatrix(CorpusHeadline)
dtm
sparse = removeSparseTerms(dtm, 0.996) # Want ~ 200 words
sparse
inspect(sparse[1:1,]) # Gives you all the columns
HeadlineWords = as.data.frame(as.matrix(sparse))
# Ensure variable names are okay for R:
colnames(HeadlineWords) = make.names(colnames(HeadlineWords))
HeadlineWordsTrain = head(HeadlineWords, nrow(ntrain))
HeadlineWordsTest = tail(HeadlineWords, nrow(ntest))
CorpusAbstract = Corpus(VectorSource(c(ntrain$Abstract, ntrain$Abstract)))
CorpusAbstract = tm_map(CorpusAbstract, tolower)
CorpusAbstract = tm_map(CorpusAbstract, PlainTextDocument)
CorpusAbstract = tm_map(CorpusAbstract, removePunctuation)
CorpusAbstract = tm_map(CorpusAbstract, removeWords, stopwords("english"))
CorpusAbstract = tm_map(CorpusAbstract, stemDocument)
dtmAb = DocumentTermMatrix(CorpusAbstract)
inspect(dtmAb[1:5,])
sparseAb = removeSparseTerms(dtmAb, 0.989) # Want ~ 200 words
sparseAb
AbstractWords = as.data.frame(as.matrix(sparseAb))
inspect(sparseAb[1:1,]) # Gives you all the columns
colnames(AbstractWords) = make.names(colnames(AbstractWords))
AbstractWordsTest = tail(AbstractWords, nrow(ntest))
AbstractWordsTrain = head(AbstractWords, nrow(ntrain))
ftrain = HeadlineWordsTrain
ftest = HeadlineWordsTest
ftrain$Popular = ntrain$Popular # DEP. VAR
ftrain$WordCount = log(ntrain$WordCount) #TAKING LOG OF VALUE
ftrain$Weekday = ntrain$Weekday
ftrain$WKday = ntrain$WKday
ftrain$Question = ntrain$Question
ftrain$HourCat = ntrain$HourCat
ftrain$NewsDesk = ntrain$NewsDesk
ftrain$SectionName = ntrain$SectionName
ftrain$UniqueID = ntrain$UniqueID
ftest$WordCount = log(ntest$WordCount)
ftest$Weekday = ntest$Weekday
ftest$WKday = ntest$WKday
ftest$Question = ntest$Question
ftest$HourCat = ntest$HourCat
ftest$NewsDesk = ntest$NewsDesk
ftest$SectionName = ntest$SectionName
ftest$UniqueID = ntest$UniqueID
set.seed(144)
split = sample.split(ftrain$Popular, SplitRatio = 0.75)
anmtrain = subset(ftrain, split == TRUE)
anmtest = subset(ftrain, split == FALSE)
anmLog.model = glm(Popular ~ ., data = anmtrain,family="binomial")
anmLog.model = glm(Popular ~ WKday, Question, HourCat, SectionName , data = anmtrain,family="binomial")
table(anmtrain$SectionName)
anmLog.model = glm(Popular ~ WKday Question HourCat SectionName , data = anmtrain,family="binomial")
anmLog.model = glm(Popular ~ WKday + Question + HourCat +  SectionName , data = anmtrain,family="binomial")
anmLog.pred = predict(anmLog.model, newdata = anmtest, type="response")
anmLog.model = glm(Popular ~ WKday + Question + HourCat  , data = anmtrain,family="binomial")
anmLog.pred = predict(anmLog.model, newdata = anmtest, type="response")
levels(anmtest$SectionName)
levels(anmtrain$SectionName)
temp = c(ntrain$SectionName, ntest$SectionName)
temp = as.factor(temp)
ntrain$SectionName = temp[1:nrow(ntrain)]
ntest$SectionName = temp[nrow(ntrain)+1:length(temp)]
ntest$SectionName = temp[(nrow(ntrain)+1):length(temp)]
ftrain$SectionName = ntrain$SectionName
ftest$SectionName = ntest$SectionName
set.seed(144)
split = sample.split(ftrain$Popular, SplitRatio = 0.75)
anmtrain = subset(ftrain, split == TRUE)
anmtest = subset(ftrain, split == FALSE)
anmLog.model = glm(Popular ~ WKday + Question + HourCat + SectionName , data = anmtrain,family="binomial")
anmLog.pred = predict(anmLog.model, newdata = anmtest, type="response")
levels(anmtest$SectionName)
levels(anmtrain$SectionName)
anmLog.pred = predict(anmLog.model, type = "response")
table(anmLog.pred>0.5)
library(ROCR)
anmLog.ROCR = prediction(anmLog.pred,anmtest$Popular)
table(anmtrain$Popular)
4079/(4079+820)
table(anmLog.pred>0.5)
(4109)/sum(4109+695) # accuracy 0.5419578
anmLog.model = glm(Popular ~ Question + HourCat + SectionName , data = anmtrain,family="binomial")
anmLog.pred = predict(anmLog.model, type = "response")
table(anmLog.pred>0.5)
anmLog.model = glm(Popular ~ HourCat + SectionName , data = anmtrain,family="binomial")
anmLog.pred = predict(anmLog.model, type = "response")
table(anmLog.pred>0.5)
anmLog.model = glm(Popular ~ ., data = anmtrain,family="binomial")
sparse = removeSparseTerms(dtm, 0.997) # Want ~ 200 words
sparse
sparse = removeSparseTerms(dtm, 0.99) # Want ~ 200 words
sparse
HeadlineWords = as.data.frame(as.matrix(sparse))
colnames(HeadlineWords) = make.names(colnames(HeadlineWords))
HeadlineWordsTrain = head(HeadlineWords, nrow(ntrain))
HeadlineWordsTest = tail(HeadlineWords, nrow(ntest))
ftrain = HeadlineWordsTrain
ftest = HeadlineWordsTest
ftrain$Popular = ntrain$Popular # DEP. VAR
ftrain$WordCount = log(ntrain$WordCount) #TAKING LOG OF VALUE
ftrain$Weekday = ntrain$Weekday
ftrain$WKday = ntrain$WKday
ftrain$Question = ntrain$Question
ftrain$HourCat = ntrain$HourCat
ftrain$NewsDesk = ntrain$NewsDesk
ftrain$SectionName = ntrain$SectionName
ftrain$UniqueID = ntrain$UniqueID
# TEST
# No dep var
ftest$WordCount = log(ntest$WordCount)
ftest$Weekday = ntest$Weekday
ftest$WKday = ntest$WKday
ftest$Question = ntest$Question
ftest$HourCat = ntest$HourCat
ftest$NewsDesk = ntest$NewsDesk
ftest$SectionName = ntest$SectionName
ftest$UniqueID = ntest$UniqueID
set.seed(144)
split = sample.split(ftrain$Popular, SplitRatio = 0.75)
anmtrain = subset(ftrain, split == TRUE)
anmtest = subset(ftrain, split == FALSE)
anmLog.model = glm(Popular ~ ., data = anmtrain,family="binomial")
anmLog.model = glm(Popular ~ . - SectionName, data = anmtrain,family="binomial")
anmLog.model = glm(Popular ~ . - SectionName - HourCat, data = anmtrain,family="binomial")
anmLog.model = glm(Popular ~ . - SectionName - HourCat - NewsDesk, data = anmtrain,family="binomial")
anmLog.model = glm(Popular ~ . - SectionName - HourCat - NewsDesk - SectionName, data = anmtrain,family="binomial")
anmLog.model = glm(Popular ~ . - SectionName - HourCat - NewsDesk - SectionName - WordCount, data = anmtrain,family="binomial")
anmLog.model = glm(Popular ~ . - SectionName - HourCat - NewsDesk - WordCount, data = anmtrain,family="binomial")
anmLog.model = glm(Popular ~ . - SectionName - HourCat - WordCount, data = anmtrain,family="binomial")
anmLog.model = glm(Popular ~ . - HourCat - WordCount, data = anmtrain,family="binomial")
anmLog.model = glm(Popular ~ . - WordCount, data = anmtrain,family="binomial")
anmLog.model = glm(Popular ~ ., data = anmtrain,family="binomial")
ftrain$WordCount = log(ntrain$WordCount + 50) #TAKING LOG OF VALUE
ftest$WordCount = log(ntest$WordCount + 50)
set.seed(144)
split = sample.split(ftrain$Popular, SplitRatio = 0.75)
anmtrain = subset(ftrain, split == TRUE)
anmtest = subset(ftrain, split == FALSE)
anmLog.model = glm(Popular ~ ., data = anmtrain,family="binomial")
anmLog.pred = predict(anmLog.model, type = "response")
table(anmtrain$Popular)
table(anmLog.pred>0.5)
(4170)/sum(41070+634) # accuracy 0.8553289 - better than the baseline, but not that good.
(4170)/sum(4170+634) # accuracy 0.8553289 - better than the baseline, but not that good.
anmLog.pred = predict(anmLog.model, newdata = anmtest, type="response")
temp = c(ntrain$NewsDesk, ntest$NewsDesk)
temp = as.factor(temp)
ntrain$NewsDesk = temp[1:nrow(ntrain)]
ntest$NewsDesk = temp[(nrow(ntrain)+1):length(temp)]
ftrain$NewsDesk = ntrain$NewsDesk
ftest$NewsDesk = ntest$NewsDesk
set.seed(144)
split = sample.split(ftrain$Popular, SplitRatio = 0.75)
anmtrain = subset(ftrain, split == TRUE)
anmtest = subset(ftrain, split == FALSE)
anmLog.model = glm(Popular ~ ., data = anmtrain,family="binomial")
anmLog.pred = predict(anmLog.model, type = "response")
table(anmLog.pred>0.5)
anmLog.pred = predict(anmLog.model, newdata = anmtest, type="response")
summary(anmLog.model)
ntrain$HourCat = cut(ntrain$Hour, c(0,7,12,18,24))
table(ntrain$HourCat)
ntest$HourCat = cut(ntest$Hour, c(0,7,12,18,24))
table(ntest$HourCat)
ftrain$HourCat = ntrain$HourCat
ftest$HourCat = ntest$HourCat
set.seed(144)
split = sample.split(ftrain$Popular, SplitRatio = 0.75)
anmtrain = subset(ftrain, split == TRUE)
anmtest = subset(ftrain, split == FALSE)
table(anmtrain$Popular)
4079/(4079+820) #Baseline Accuracy is 0.8326189
anmLog.model = glm(Popular ~ ., data = anmtrain,family="binomial")
anmLog.pred = predict(anmLog.model, type = "response")
summary(anmLog.model)
table(anmLog.pred>0.5)
(4171)/sum(4171+633) # accuracy 0.8680266 - better than the baseline, but not that good.
anmLog.model = glm(Popular ~ . - SectionName, data = anmtrain,family="binomial")
anmLog.pred = predict(anmLog.model, type = "response")
table(anmLog.pred>0.5)
(4231)/sum(4231+573) # accuracy 0.8553289 - better than the baseline, but not that good.
words = c("X2014","X2015")
anmLog.model = glm(Popular ~ . - SectionName - words, data = anmtrain,family="binomial")
anmLog.model = glm(Popular ~ . - SectionName - X2014 - X2015, data = anmtrain,family="binomial")
anmLog.pred = predict(anmLog.model, type = "response")
table(anmLog.pred>0.5)
summary(anmLog.model)
anmLog.model = glm(Popular ~ . - SectionName, data = anmtrain,family="binomial")
anmLog.pred = predict(anmLog.model, type = "response")
summary(anmLog.model)
anmLog.model = glm(Popular ~ . - SectionName - X2014 - X2015 - can - make - week - will - pari - daili - springsumm - small, data = anmtrain,family="binomial")
anmLog.pred = predict(anmLog.model, type = "response")
table(anmLog.pred>0.5)
anmLog.model = glm(Popular ~ . - SectionName - X2014 - X2015 - can - make - week - will - pari - daili - springsumm - small - NewsDesk3 - NewsDesk5 - NewsDesk12, data = anmtrain,family="binomial")
anmtrain$NewsDesk3
anmtrain$NewsDesk
(4229)/sum(4229+575) # accuracy 0.8807244 - better than the baseline, by about .05
anmLog.model = glm(Popular ~ . - SectionName - Weekday, data = anmtrain,family="binomial")
anmLog.pred = predict(anmLog.model, type = "response")
summary(anmLog.model)
table(anmLog.pred>0.5)
anmLog.model = glm(Popular ~ . - SectionName - Weekday, data = anmtrain,family="binomial")
anmLog.pred = predict(anmLog.model, type = "response")
summary(anmLog.model)
table(anmLog.pred>0.5)
anmLog.model = glm(Popular ~ . - SectionName - Weekday - NewsDesk, data = anmtrain,family="binomial")
anmLog.pred = predict(anmLog.model, type = "response")
summary(anmLog.model)
table(anmLog.pred>0.5)
(4519)/sum(4519+285) # accuracy 0.8807244 - SAME AS MODEL 2
anmLog.model = glm(Popular ~ . - SectionName - Weekday, data = anmtrain,family="binomial")
anmLog.pred = predict(anmLog.model, type = "response")
MySubmission = data.frame(UniqueID = NewsTest$UniqueID, Probability1 = anmLog.pred)
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmLog.pred)
anmLog.model = glm(Popular ~ . - SectionName - Weekday, data = ftrain,family="binomial")
anmLog.pred = predict(anmLog.model, newdata = ftest, type ="response")
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmLog.pred)
write.csv(MySubmission, "SubmissionHeadlineLog.csv", row.names=FALSE)
View(MySubmission)
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmLog.pred)
write.csv(MySubmission, "SubmissionHeadlineLog.csv", row.names=FALSE)
setwd("~/Desktop/R_EDX/KAGGLE")
ntrain = read.csv("NYTimesBlogTrain.csv", stringsAsFactors=FALSE)
ntest = read.csv("NYTimesBlogTest.csv", stringsAsFactors=FALSE)
# Deal with the factor problem
temp = c(ntrain$SectionName, ntest$SectionName)
temp = as.factor(temp)
ntrain$SectionName = temp[1:nrow(ntrain)]
ntest$SectionName = temp[(nrow(ntrain)+1):length(temp)]
temp = c(ntrain$NewsDesk, ntest$NewsDesk)
temp = as.factor(temp)
ntrain$NewsDesk = temp[1:nrow(ntrain)]
ntest$NewsDesk = temp[(nrow(ntrain)+1):length(temp)]
ntrain$PubDate = strptime(ntrain$PubDate, "%Y-%m-%d %H:%M:%S")
ntest$PubDate = strptime(ntest$PubDate, "%Y-%m-%d %H:%M:%S")
# Setup new Variable - Weekday
ntrain$Weekday = ntrain$PubDate$wday
ntrain$WKday <- ntrain$Weekday == 0 | ntrain$Weekday == 6
ntest$Weekday = ntest$PubDate$wday
ntest$WKday <- ntest$Weekday == 0 | ntest$Weekday == 6
str(ntrain)
# Setup New Variable - Is there a Question Mark??
ntrain$Question <- grepl("\\?",ntrain$Headline)
ntest$Question <- grepl("\\?",ntest$Headline)
# Setup New Variable - Hour
ntrain$Hour = ntrain$PubDate$hour
ntest$Hour = ntest$PubDate$hour
# New Variable for Prime Reading Hours
# Three levels: Midnight to 7am; 7am to 6pm, 6pm to midnight
ntrain$HourCat = cut(ntrain$Hour, c(0,7,12,18,24))
table(ntrain$HourCat)
ntest$HourCat = cut(ntest$Hour, c(0,7,12,18,24))
table(ntest$HourCat)
# Five levels
# ntrain$HourCat = cut(ntrain$Hour, c(0,6,11,15,18,22,24))
# table(ntrain$HourCat)
# ntest$HourCat = cut(ntest$Hour, c(0,6,11,15,18,22,24))
# table(ntest$HourCat)
#******************* Text - Headline ***********************
library(tm)
# HEADLINE
# Note that we are creating a corpus out of the training and testing data.
CorpusHeadline = Corpus(VectorSource(c(ntrain$Headline, ntest$Headline)))
CorpusHeadline = tm_map(CorpusHeadline, tolower)
# Remember this extra line is needed after running the tolower step:
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
# Convert our corpus to a DocumentTermMatrix, remove sparse terms, and turn it into a data frame.
dtm = DocumentTermMatrix(CorpusHeadline)
dtm
# MODIFYIABLE - SPARSE TERMS THRESHOLD:
sparse = removeSparseTerms(dtm, 0.99) # Want ~ 200 words
sparse
inspect(sparse[1:1,]) # Gives you all the columns
HeadlineWords = as.data.frame(as.matrix(sparse))
# Ensure variable names are okay for R:
colnames(HeadlineWords) = make.names(colnames(HeadlineWords))
# Split the observations back into the training set and testing set.
HeadlineWordsTrain = head(HeadlineWords, nrow(ntrain))
HeadlineWordsTest = tail(HeadlineWords, nrow(ntest))
# To do this, we can use the head and tail functions in R.
ftrain = HeadlineWordsTrain
ftest = HeadlineWordsTest
ftrain$Popular = ntrain$Popular # DEP. VAR
ftrain$WordCount = log(ntrain$WordCount + 50) #TAKING LOG OF VALUE
ftrain$Weekday = ntrain$Weekday
ftrain$WKday = ntrain$WKday
ftrain$Question = ntrain$Question
ftrain$HourCat = ntrain$HourCat
ftrain$NewsDesk = ntrain$NewsDesk
ftrain$SectionName = ntrain$SectionName
ftrain$UniqueID = ntrain$UniqueID
ftest$WordCount = log(ntest$WordCount + 50)
ftest$Weekday = ntest$Weekday
ftest$WKday = ntest$WKday
ftest$Question = ntest$Question
ftest$HourCat = ntest$HourCat
ftest$NewsDesk = ntest$NewsDesk
ftest$SectionName = ntest$SectionName
ftest$UniqueID = ntest$UniqueID
set.seed(144)
table(ftrain$Popular)
5439/(5439+1093) #Baseline Accuracy is 0.8326189
anmLog.model = glm(Popular ~ . - SectionName - Weekday, data = ftrain,family="binomial")
anmLog.pred = predict(anmLog.model, newdata = ftest, type ="response")
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmLog.pred)
View(MySubmission)
MySubmission = data.frame(UniqueID = ftest$UniqueID, Probability1 = anmLog.pred)
write.csv(MySubmission, "SubmissionHeadlineLog.csv", row.names=FALSE)
MySubmission = data.frame(UniqueID = NewsTest$UniqueID, Probability1 = anmLog.pred)
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmLog.pred)
write.csv(MySubmission, "SubmissionHeadlineLog.csv", row.names=FALSE)
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmLog.pred)
write.csv(MySubmission, "SubmissionHeadlineLog.csv", row.names=FALSE)
View(MySubmission)
NewsTrain = read.csv("NYTimesBlogTrain.csv", stringsAsFactors=FALSE)
NewsTest = read.csv("NYTimesBlogTest.csv", stringsAsFactors=FALSE)
CorpusHeadline = Corpus(VectorSource(c(NewsTrain$Headline, NewsTest$Headline)))
# You can go through all of the standard pre-processing steps like we did in Unit 5:
CorpusHeadline = tm_map(CorpusHeadline, tolower)
# Remember this extra line is needed after running the tolower step:
CorpusHeadline = tm_map(CorpusHeadline, PlainTextDocument)
CorpusHeadline = tm_map(CorpusHeadline, removePunctuation)
CorpusHeadline = tm_map(CorpusHeadline, removeWords, stopwords("english"))
CorpusHeadline = tm_map(CorpusHeadline, stemDocument)
# Now we are ready to convert our corpus to a DocumentTermMatrix, remove sparse terms, and turn it into a data frame.
# We selected one particular threshold to remove sparse terms, but remember that you can try different numbers!
dtm = DocumentTermMatrix(CorpusHeadline)
sparse = removeSparseTerms(dtm, 0.99)
HeadlineWords = as.data.frame(as.matrix(sparse))
# Let's make sure our variable names are okay for R:
colnames(HeadlineWords) = make.names(colnames(HeadlineWords))
# To do this, we can use the head and tail functions in R.
# The head function takes the first "n" rows of HeadlineWords (the first argument to the head function), where "n" is specified by the second argument to the head function.
# So here we are taking the first nrow(NewsTrain) observations from HeadlineWords, and putting them in a new data frame called "HeadlineWordsTrain"
HeadlineWordsTrain = head(HeadlineWords, nrow(NewsTrain))
# So here we are taking the last nrow(NewsTest) observations from HeadlineWords, and putting them in a new data frame called "HeadlineWordsTest"
HeadlineWordsTest = tail(HeadlineWords, nrow(NewsTest))
# The tail function takes the last "n" rows of HeadlineWords (the first argument to the tail function), where "n" is specified by the second argument to the tail function.
# Now we need to split the observations back into the training set and testing set.
HeadlineWordsTrain$Popular = NewsTrain$Popular
HeadlineWordsTrain$WordCount = NewsTrain$WordCount
HeadlineWordsTest$WordCount = NewsTest$WordCount
HeadlineWordsLog = glm(Popular ~ ., data=HeadlineWordsTrain, family=binomial)
PredTest = predict(HeadlineWordsLog, newdata=HeadlineWordsTest, type="response")
MySubmission = data.frame(UniqueID = NewsTest$UniqueID, Probability1 = PredTest)
write.csv(MySubmission, "SubmissionHeadlineLog.csv", row.names=FALSE)
View(MySubmission)
anmLog.model = glm(Popular ~ . - SectionName - Weekday, data = ftrain,family=binomial)
anmLog.pred = predict(anmLog.model, newdata = ftest, type ="response")
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmLog.pred)
View(MySubmission)
rm(MySubmission)
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmLog.pred)
View(MySubmission)
MySubmission = data.frame(UniqueID = NewsTest$UniqueID, Probability1 = PredTest)
View(MySubmission)
rm(MySubmission)
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmLog.pred)
View(MySubmission)
MySubmission = data.frame(UniqueID = NewsTest$UniqueID, Probability1 = PredTest)
View(MySubmission)
library(rpart)
library(rpart.plot)
anmCART.model = rpart(Popular ~., data=ftrain, method="class") # Classification model - no min bucket or CP (using defaults)
prp(anmCART.model)
anmCART.pred = predict(anmCART.model, newdata=ftest, type="class")
table(anmCART.pred)
(1652)/nrow(ftrain) # Accruacy 0.8788732
(1652)/nrow(ftest) # Accruacy 0.8788732
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmCART.pred)
write.csv(MySubmission, "SubmissionHeadlineLog.csv", row.names=FALSE)
View(MySubmission)
library(randomForest)
set.seed(123)
anmRF.model = randomForest(Popular ~., data=ftrain) # default settings
anmRF.pred = predict(anmRF.model, newdata=ftest)
table(anmRF.pred)
anmRF.model = randomForest(Popular ~., data=ftrain)
?randomForest
table(ftrain$Popular)
anmRF.model = randomForest(Popular ~., data=ftrain,na.action = na.omit)
y
anmRF.pred = predict(anmRF.model, newdata=ftest)
table(anmRF.pred)
anmRF.pred = predict(anmRF.model, newdata=ftest,type="prob")[,2]
anmRF.pred = predict(anmRF.model, newdata=ftest,type="prob")
anmRF.pred = predict(anmRF.model, newdata=ftest)[,2]
anmRF.pred = predict(anmRF.model, newdata=ftest)[,1]
anmRF.model = randomForest(Popular ~., data=ftrain,na.action = na.omit)[,1]
yes
anmRF.pred = predict(anmRF.model, newdata=ftest)[,1]
anmRF.pred = predict(anmRF.model, newdata=ftest)[,2]
anmRF.pred = predict(anmRF.model, newdata=ftest)
table(anmRF.pred)
rm(MySubmission)
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmRF.pred)
View(MySubmission)
anmLog.model = glm(Popular ~ . - SectionName - Weekday - NewsDesk, data = anmtrain,family="binomial")
anmLog.model = glm(Popular ~ . - SectionName - Weekday - NewsDesk, data = ftrain,family="binomial")
anmLog.pred = predict(anmLog.model,data=ftest, type = "response")
summary(anmLog.model)
table(anmLog.pred>0.5)
MySubmission = data.frame(UniqueID = ftest$UniqueID, Probability1 = anmLog.pred)
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmLog.pred)
anmLog.model = glm(Popular ~ . - SectionName - Weekday - NewsDesk, data = ftrain,family="binomial")
anmLog.pred = predict(anmLog.model,data=ftest, type = "response")
anmLog.pred = predict(anmLog.model,newdata=ftest, type = "response")
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmLog.pred)
write.csv(MySubmission, "SubmissionHeadlineLog.csv", row.names=FALSE)
View(MySubmission)
ftest[1168]
ftest[1168,]
ftest[1169,]
ntest$HourCat
ntest$Hour
ntest$HourCat = cut(ntest$Hour, c(-1,7,12,18,24))
ntest$HourCat
ntrain$HourCat = cut(ntrain$Hour, c(-1,7,12,18,25))
table(ntrain$HourCat)
ntest$HourCat = cut(ntest$Hour, c(-1,7,12,18,25))
table(ntest$HourCat)
ftrain$HourCat = ntrain$HourCat
ftest$HourCat = ntest$HourCat
anmLog.model = glm(Popular ~ . - SectionName - Weekday, data = ftrain,family="binomial")
anmLog.pred = predict(anmLog.model, type = "response")
anmLog.pred = predict(anmLog.model, newdata = ftest, type ="response")
summary(anmLog.model)
anmLog.model = glm(Popular ~ . - SectionName - Weekday - X2014 - X2015 - can - make - week - will - pari - daili - springsumm - small - NewsDesk3 - NewsDesk5 - NewsDesk12, data = ftrain,family="binomial")
anmLog.model = glm(Popular ~ . - SectionName - Weekday - X2014 - X2015 - can - make - week - will - pari - daili - springsumm - small , data = ftrain,family="binomial")
anmLog.pred = predict(anmLog.model, newdata= ftest, type = "response")
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmLog.pred)
write.csv(MySubmission, "SubmissionHeadlineLog.csv", row.names=FALSE)
anmRF.model = randomForest(Popular ~., data=ftrain)
anmRF.pred = predict(anmRF.model, newdata=ftest)
rm(MySubmission)
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmRF.pred)
View(MySubmission)
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmRF.pred)
write.csv(MySubmission, "SubmissionHeadlineLog.csv", row.names=FALSE)
summary(anmLog.model)
anmRF.model = randomForest(Popular ~ . - SectionName - Weekday - X2014 - X2015 - can - make - week - will - pari - daili - springsumm - small , data=ftrain)
anmRF.pred = predict(anmRF.model, newdata=ftest)
rm(MySubmission)
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmRF.pred)
write.csv(MySubmission, "SubmissionHeadlineLog.csv", row.names=FALSE)
kfold(ftrain,randomForest,pargs=list(type="prob"),nodesize=1,ntree=500)
anmLog.model = glm(Popular ~ . - SectionName - Weekday - NewsDesk, data = ftrain,family="binomial")
anmLog.pred = predict(anmLog.model,newdata=ftest, type = "response")
summary(anmLog.model)
MySubmission = data.frame(UniqueID = ntest$UniqueID, Probability1 = anmLog.pred)
write.csv(MySubmission, "SubmissionHeadlineLog.csv", row.names=FALSE)
