strwrap(corpus[[1]])
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus,removePunctuation)
corpus = tm_map(corpus,removeWrods, stopwords("english"))
corpus = tm_map(corpus,stemDocument)
strwrap(corpus[[1]])
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus,removePunctuation)
corpus = tm_map(corpus,removeWords, stopwords("english"))
corpus = tm_map(corpus,stemDocument)
strwrap(corpus[[1]])
library(tm)
corpus = Corpus(VectorSource(emails$email))
strwrap(corpus[[1]])
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus,removePunctuation)
corpus = tm_map(corpus,removeWords, stopwords("english"))
corpus = tm_map(corpus,stemDocument)
strwrap(corpus[[1]])
dtm = DocumentTermMatrix(corpus)
dtm
dtm = removeSparseTerms(dtm, 0.97)
dtm
labeledTerms = as.data.frame(as.matrix(dtm))
labeledTerms$responsive = emails$responsive
str(labeledTerms)
library(caTools)
set.seed(144)
spl = sample.split(labeledTerms$responsive, 0.7)
train = subset(labeledTerms, spl == TRUE)
test = subset(labeledTerms, spl == FALSE)
library(rpart)
library(rpart.plot)
emailCART = rpart(responsive ~., data=train,method = "class")
prp(emailCART)
predCART = predict(emailCART, newdata=test)
predCART[1:10,]
pred.prob = pred[,2]
pred.prob = predCART[,2]
table(test$responsive, pred.prob >= 0.5)
(195+25)/nrow(test)
table(test$responsive)
215/(215+42)
library(ROCR)
predROCR = predication(pred.prob, test$responsive)
predROCR = prediction(pred.prob, test$responsive)
perfROCR = performance(predROCR, "tpr", "fpf")
perfROCR = performance(predROCR, "tpr", "fpr")
plot(perfROCR, colorize = TRUE)
performance(predROCR, "auc")@y.values
andyfun = subset(emails, responsive == TRUE)
summary(andyfun)
length(andyfun)
str(andyfun)
table(emails$responsive)
str(andyfun)
andyfun = subset(labeledTerms, responsive == TRUE)
str(andyfun)
andyfun = subset(corpus, responsive == TRUE)
wiki = read.csv("wiki.csv", stringsAsFactors=FALSE)
summary(wiki)
table(wiki$vandal)
table(wiki$Vandal)
wiki$Vandal = as.factor(wiki$Vandal)
table(wiki$Vandal)
library(tm)
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded = tm_map(corpusAdded,removeWords, stopwords("english"))
corpusAdded = tm_map(corpusAdded,stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
dtmAdded
dtmAdded[1:10,]
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
sparseAdded
wordsAdded = as.data.frame(matrix(sparseAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded)) # Prepend all words with the letter A
corpusRemoved = Corpus(VectorSource(wiki$Removed))
# Remove stopwords
corpusRemoved = tm_map(corpusRemoved,removeWords, stopwords("english"))
corpusRemoved = tm_map(corpusRemoved,stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
dtmRemoved
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
sparseRemoved
wordsRemoved = as.data.frame(matrix(sparseRemoved))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved)) # Prepend all words with the letter A
wordsRemoved
str(wordsRemoved)
sparseRemoved
ncol(wordsRemoved)
wikiWords = cbind(wordsAdded, wordsRemoved)
wiki = read.csv("wiki.csv", stringsAsFactors=FALSE)
wiki$Vandal = as.factor(wiki$Vandal)
table(wiki$Vandal)
library(tm)
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded = tm_map(corpusAdded,removeWords, stopwords("english"))
corpusAdded = tm_map(corpusAdded,stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
dtmAdded
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
sparseAdded
wordsAdded = as.data.frame(matrix(sparseAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded)) # Prepend all words with the letter A
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved,removeWords, stopwords("english"))
dtmRemoved = DocumentTermMatrix(corpusRemoved)
dtmRemoved
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
sparseRemoved
wordsRemoved = as.data.frame(matrix(sparseRemoved))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved)) # Prepend all words with the letter A
ncol(wordsRemoved)
wikiWords = cbind(wordsAdded, wordsRemoved)
str(wordsRemoved)
wordsRemoved
table(wordsRemoved)
View(wikiWords)
wikiWords$Vandal = wiki$Vandal
set.seed(123)
spl = sample.split(wikiWords$Vandal, 0.7)
train = subset(wikiWords, spl == TRUE)
test = subset(wikiWords, spl == FALSE)
table(test$Vandal)
102638 / nrow(test)
wiki = read.csv("wiki.csv", stringsAsFactors=FALSE)
wiki$Vandal = as.factor(wiki$Vandal)
table(wiki$Vandal)
str(wiki)
corpusAdded = Corpus(VectorSource(wiki$Added))
library(tm)
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded = tm_map(corpusAdded,removeWords, stopwords("english"))
corpusAdded = tm_map(corpusAdded,stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
wordsAdded = as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded)) # Prepend all words with the letter A
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved,removeWords, stopwords("english"))
corpusRemoved = tm_map(corpusRemoved,stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved)) # Prepend all words with the letter A
str(wordsRemoved)
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords$Vandal = wiki$Vandal
set.seed(123)
spl = sample.split(wikiWords$Vandal, 0.7)
train = subset(wikiWords, spl == TRUE)
test = subset(wikiWords, spl == FALSE)
table(test$Vandal)
618 / nrow(test) # Accruacy 0.5317342
vandalCART = rpart(Vandal ~., data=train,method = "class")
prp(vandalCART)
predCART = predict(vandalCART, newdata=test) # automatically uses .5 threshold
table(test$Vandal,predCART)
pred.prob = predCART[,2]
table(test$Vandal,pred.prob>=.50)
(618+12)/nrow(test)
predCART = predict(vandalCART, newdata=test, type = "class") # automatically uses .5 threshold
table(test$Vandal,predCART>=.50)
table(test$Vandal,predCART)
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE), 1, 0)
table(wikiWords2$HTTP)
wikiTrain2 = subset(wikiWords2, spl==TRUE)
wikiTest2 = subset(wikiWords2, spl==FALSE)
newCART = rpart(vandal~.,data=train,method="class")
newCART = rpart(vandal~.,data=wikiTrain2,method="class")
newCART = rpart(Vandal~.,data=wikiTrain2,method="class")
pred.newCART = predict(newCART,newdata=wikiTest2,type="class")
table(test$Vandal,pred.newCART)
table(wikiTest2$Vandal,pred.newCART)
(609+57)/nrow(wikiTest2)
NumWordsAdded = sum(dtmAdded)
NumWordsAdded
NumWordsRemoved = sum(dtmRemoved)
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
summary(wikiWords2)
mean(wikiWords2$NumWordsAdded)
new2CART = rpart(Vandal~.,data=wikiTrain2,method="class")
pred.new2CART = predict(newCART2,newdata=wikiTest2,type="class")
pred.new2CART = predict(new2CART,newdata=wikiTest2,type="class")
table(wikiTest2$Vandal,pred.new2CART)
(609+57)/nrow(wikiTest2) # Accuracy 0.5726569
wikiTrain2 = subset(wikiWords2, spl==TRUE)
wikiTest2 = subset(wikiWords2, spl==FALSE)
new2CART = rpart(Vandal~.,data=wikiTrain2,method="class")
pred.new2CART = predict(new2CART,newdata=wikiTest2,type="class")
table(wikiTest2$Vandal,pred.new2CART)
(514+248)/nrow(wikiTest2) # Accuracy 0.5726569
wikiTrain3 = subset(wikiWords2, spl==TRUE)
wikiTest3 = subset(wikiWords2, spl==FALSE)
new2CART = rpart(Vandal~.,data=wikiTrain3,method="class")
pred.new2CART = predict(new2CART,newdata=wikiTest3,type="class")
table(wikiTest3$Vandal,pred.new2CART)
(514+248)/nrow(wikiTest3) # Accuracy 0.6552021
wikiWords3 = wikiWords2
wikiWords3$Minor = wiki$Minor
wikiWords3$Loggedin = wiki$Loggedin
wikiTrain4 = subset(wikiWords3, spl==TRUE)
wikiTest4 = subset(wikiWords3, spl==FALSE)
new3CART = rpart(Vandal~.,data=wikiTrain4,method="class")
pred.new3CART = predict(new3CART,newdata=wikiTest4,type="class")
table(wikiTest4$Vandal,pred.new3CART)
(595+241)/nrow(wikiTest4) # Accuracy 0.6552021
prp(new3CART)
trials = read.csv("clinical_trial.csv", stringsAsFactors=FALSE)
summary(trials)
str(trials)
which.max?
?which.max
?nchar
nchar(trails$abstract)
nchar(trials$abstract)
sort(nchar(trials$abstract))
nchar(trials$abstract)
which.max(trials$abstract)
max(nchar(trials$abstract))
summary(nchar(trials$abstract))
table(nchar(trials$abstract) == 0)
summary(nchar(trials$abstract))
summary(nchar(trials$title))
which.min(nchar(trials$title))
trials$title[1258]
corpusTitle = Corpus(VectorSource(trials$title))
corpusTitle = tm_map(corpusTitle, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusTitle = tm_map(corpusTitle,removePunctuation)
corpusTitle = tm_map(corpusTitle,removeWords, stopwords("english"))
corpusTitle = tm_map(corpusTitle,stemDocument)
corpusAbstract = Corpus(VectorSource(trials$abstract))
corpusAbstract = tm_map(corpusAbstract, tolower)
corpusAbstract = tm_map(corpusAbstract, PlainTextDocument)
corpusAbstract = tm_map(corpusAbstract,removePunctuation)
corpusAbstract = tm_map(corpusAbstract,removeWords, stopwords("english"))
corpusAbstract = tm_map(corpusAbstract,stemDocument)
dtmTitle = DocumentTermMatrix(corpusTitle)
sparseTitle = removeSparseTerms(dtmTitle, 0.95)
fTitle = as.data.frame(as.matrix(sparseTitle))
dtmAbstract = DocumentTermMatrix(corpusAbstract)
sparseAbstract = removeSparseTerms(dtmAbstract, 0.95)
findFreqTerms(sparseAbstract,5)
findFreqTerms(sparseAbstract,15)
findFreqTerms(sparseAbstract,55)
findFreqTerms(sparseAbstract,155)
findFreqTerms(sparseAbstract,255)
findFreqTerms(sparseAbstract,455)
findFreqTerms(sparseAbstract,855)
findFreqTerms(sparseAbstract,1055)
findFreqTerms(sparseAbstract,2055)
findFreqTerms(sparseAbstract,3000)
findFreqTerms(sparseAbstract,4000)
colSums(fAbstract)
dtmTitle = DocumentTermMatrix(corpusTitle)
sparseTitle = removeSparseTerms(dtmTitle, 0.95)
fTitle = as.data.frame(as.matrix(sparseTitle))
dtmAbstract = DocumentTermMatrix(corpusAbstract)
sparseAbstract = removeSparseTerms(dtmAbstract, 0.95)
fAbstract = as.data.frame(as.matrix(sparseAbstract))
colSums(fAbstract)
sort(colSums(fAbstract))
dfTitle = as.data.frame(as.matrix(sparseTitle))
dfAbstract = as.data.frame(as.matrix(sparseAbstract))
colnames(dfTitle) = paste0("T", colnames(dfTitle))
colnames(dfAbstract) = paste0("A", colnames(dfAbstract))
dtm = cbind(dtmTitle, dtmAbstract) # Combine into single DF
dtm
dtm$trial = trials$trial
ncols(dtm)
ncol(dtm)
corpusTitle = Corpus(VectorSource(trials$title))
corpusTitle = tm_map(corpusTitle, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusTitle = tm_map(corpusTitle,removePunctuation)
corpusTitle = tm_map(corpusTitle,removeWords, stopwords("english"))
corpusTitle = tm_map(corpusTitle,stemDocument)
corpusAbstract = Corpus(VectorSource(trials$abstract))
corpusAbstract = tm_map(corpusAbstract, tolower)
corpusAbstract = tm_map(corpusAbstract, PlainTextDocument)
corpusAbstract = tm_map(corpusAbstract,removePunctuation)
corpusAbstract = tm_map(corpusAbstract,removeWords, stopwords("english"))
corpusAbstract = tm_map(corpusAbstract,stemDocument)
#Build a Document Term Matrix
dtmTitle = DocumentTermMatrix(corpusTitle)
sparseTitle = removeSparseTerms(dtmTitle, 0.95)
dfTitle = as.data.frame(as.matrix(sparseTitle))
dtmAbstract = DocumentTermMatrix(corpusAbstract)
sparseAbstract = removeSparseTerms(dtmAbstract, 0.95)
dfAbstract = as.data.frame(as.matrix(sparseAbstract))
sort(colSums(dfAbstract)) # Sort all the terms
colnames(dfTitle) = paste0("T", colnames(dfTitle)) # pastes a T at the beginning of each column name for dtmTitle, which are the variable names.
colnames(dfAbstract) = paste0("A", colnames(dfAbstract))
dtm = cbind(dtmTitle, dtmAbstract) # Combine into single DF
dtm$trial = trials$trial
ncol(dtm)
dtm = cbind(dfTitle, dfAbstract) # Combine into single DF
colnames(dfTitle) = paste0("T", colnames(dfTitle)) # pastes a T at the beginning of each column name for dtmTitle, which are the variable names.
colnames(dfAbstract) = paste0("A", colnames(dfAbstract))
dtm = cbind(dfTitle, dfAbstract) # Combine into single DF
dtm$trial = trials$trial
ncol(dtm)
library(caTools)
set.seed(144)
spl = sample.split(dtm$trial, 0.7)
train = subset(dtm, spl == TRUE)
test = subset(dtm, spl == FALSE)
table(train$trial)
730/nrow(train)
trialCART = rpart(trial~.,data=train,method="class")
prp(trialCART)
predCART = predict(trialCART)
predCART[1:10,]
summary(pred.prob)
predCART = predict(trialCART)
predCART[1:10,]
pred.prob = predCART[,2] # contains our test set predicted probabilities
summary(pred.prob)
predTrain = predict(trialCART)[,2] # MEANS KEEP ONLY SECOND COLUMN
predTrain = predict(trialCART)[,2] # MEANS KEEP ONLY SECOND COLUMN
summary(predTrain)
predTrainF = predict(trialCART, newdata=test,type="class")
table(test$trial, predTrainF)
(261+162)/nrow(test)
161/(161+83)
261/(261+52) # TN / TN+FP = 0.6598361
table(test$trial, predTrainF>=.5)
table(test$trial, predTrainF>.5)
table(test$trial, predTrainF)
table(test$trial, predTrain)
table(test$trial, predTrain>0.5)
table(test$trial, predTrainF)
(261+162)/(261+162+52+53) # accuracy 0.7580645
(261+162)/(261+162+52+83) # accuracy 0.7580645
table(train$trial, predTrain >= 0.5)
(631+441)/nrow(train) # accuracy 0.7580645
(631+441)/nrow(train) # accuracy 0.8233487
441/(441+131) # TP / TP+FN = 0.6598361
631/(631+99) # TN / TN+FP = 0.8338658
predTest = predict(trialCART, newdata = test, type="class")
table(test$trail,predTest)
table(test$trail,predTest>0.5)
table(test$trial,predTest)
(261+162)/nrow(test)
predROCR = prediction(predTest, test$trial)
perfROCR = performance(predROCR, "tpr", "fpr")
library(ROCR)
predROCR = prediction(predTest, test$trial)
predTest = predict(trialCART, newdata = test, type="class")
predROCR = prediction(predTest, test$trial)
predROCR = prediction(predTest[,2], test$trial)
predROCR = prediction(predTest, test$trial)
predROCR = prediction(predTest, test$trial)
predTest = predict(trialCART, newdata = test)
predROCR = prediction(predTest, test$trial)
predTest = predict(trialCART, newdata = test, type="class")
predROCR = prediction(predTest, test$trial)
performance(predROCR, "auc")@y.values
pred.prob = predTest[,2]
pred.prob = predTest[,1]
predROCR = prediction(predTest, test$trial)
emails = read.csv("emails.csv", stringsAsFactors=FALSE)
summary(emails)
str(emails)
table(emails$spam)
which.max(nchar(emails$text))
trials$title[2651]
emails$title[2651]
emails$text[2651]
nchar(emails$text[2651])
which.min(nchar(emails$text))
# or
min(nchar(emails$text))
which(nchar(emails$text) == 13)
library(tm)
corpus = Corpus(VectorSource(emails$text))
corpus = tm_map(corpus, tolower)
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus,removePunctuation)
corpus = tm_map(corpus,removeWords, stopwords("english"))
corpus = tm_map(corpus,stemDocument)
dtm = DocumentTermMatrix(corpus)
ncol(dtm)
summary(dtm)
dtm
spdtm = removeSparseTerms(dtm, 0.95)
spdtm
emailsSparse = as.data.frame(as.matrix(spdtm))
colnames(emailsSparse) = make.names(colnames(emailsSparse))
sort(colSums(emailsSparse))
emailsSparse$spam = emails$spam
findFreqTerms(emailsSparse,5000)
findFreqTerms(spdtm,5000)
table(findFreqTerms(spdtm,5000),emails$spam)
emailsHam = subset(emails, spam == FALSE)
emailsHam
table(emailsHam)
sort(colSums(subset(emailsSparse, spam == 0)))
sort(colSums(subset(emailsSparse, spam == 1)))
emailsSparse$spam = as.factor(emailsSparse$spam)
set.seed(123)
spl = sample.split(emailsSparse$spam, splitRatio = 0.7)
spl = sample.split(emailsSparse$spam, 0.7)
train = subset(emailsSparse, spl == TRUE)
test = subset(emailsSparse, spl == FALSE)
spamLog = glm(spam~., data = train,family="binomial")
spamCART = rpart(spam~., data = train,method="class")
set.seed(123)
spamRF = randomForest(spam ~., data=train)
pred.LOG2 = predict(spamCART)[,2]
predictLOG1 = predict(spamLog, newdata=test, type="response")
pred.CART1 = predict(spamCART, newdata=test, type="class")
pred.CART2 = predict(spamCART)[,2]
predictRF1 = predict(spamRF, newdata=test)
pred.RF2 = predict(spamCART, type="prob")[,2]
pred.LOG2
length(pred.LOG2)
length(pred.LOG2<0.00001)
length(subset(pred.LOG2,<0.00001))
length(subset(pred.LOG2,pred.LOG2<0.00001))
length(subset(pred.LOG2,pred.LOG2>.99999))
4010-2447
sort(pred.LOG2)
pred.LOG2 = predict(spamLOG)[,2]
spamLog = glm(spam~., data = train,family="binomial")
pred.LOG2 = predict(spamLog)[,2]
pred.LOG2 = predict(spamLog)
pred.LOG2
length(subset(pred.LOG2,pred.LOG2<0.00001)) # 2447
length(subset(pred.LOG2,pred.LOG2>.99999)) # 0
(3056+954)-4010
summary(spamLog)
prp(spamCART)
table(train$spam, predTrainLog >0.5)
predTrainLog = predict(spamLog, type="response")
table(train$spam, predTrainLog >0.5)
(3052+954)/nrow(train) # Accuracy
predROCR = prediction(predTrainLog, test$spam)
performance(predROCR, "auc")@y.values
predictionTrainLog = prediction(predTrainLog, train$spam)
as.numeric(performance(predictionTrainLog, "auc")@y.values)
table(test$spam, predTrainCART >0.5)
predTrainCART = predict(spamCART)[,2]
table(test$spam, predTrainCART >0.5)
table(test$spam, predTrainCART)
predTrainCART = predict(spamCART)[,2]
mean(predTrainCART)
table(train$spam, predTrainCART)
table(train$spam, predTrainCART>0.5)
(2885+894)/nrow(train) # Accuracy
predictionTrainCART = prediction(predTrainCART, train$spam)
as.numeric(performance(predictionTrainCART, "auc")@y.values)
predTestRF = predict(spamRF, newdata=test)
predTrainRF = predict(spamCART, type="prob")[,2]
table(train$spam, predTrainRF>0.5)
(2885+894)/nrow(train) # Accuracy 0.942394
predTrainRF = predict(spamRF, type="prob")[,2]
table(train$spam, predTrainRF>0.5)
(3013+914)/nrow(train) # Accuracy 0.942394
predictionTrainRF = prediction(predTrainRF, train$spam)
as.numeric(performance(predictionTrainRF, "auc")@y.values)
table(test$spam, predTestLog >0.5)
predictTestLog = predict(spamLog, newdata=test, type="response")
table(test$spam, predTestLog >0.5)
table(test$spam, predictTestLog >0.5)
(1257+376)/nrow(test) # Accuracy
predictionTestLog = prediction(predictTestLog, test$spam)
as.numeric(performance(predictionTestLog, "auc")@y.values)
predTestCART
predTestCART = predict(spamCART, newdata=test, type="class")
table(test$spam, predTestCART >0.5)
table(test$spam, predTestCART)
(1228+386)/nrow(test) # Accuracy 0.9505239
predictionTestCART = prediction(predTestCART, test$spam)
predTestCART = predict(spamCART, newdata=test, type="class")
predictionTestCART = prediction(predTestCART, test$spam)
predTestRF = predict(spamRF, newdata=test)
table(test$spam, predTestRF)
(1290+385)/nrow(test) # Accuracy 0.9394645
predictionTestRF = prediction(predTestRF, test$spam)
as.numeric(performance(predictionTestRF, "auc")@y.values) #0.9627517
predictionTestRF = prediction(predTestRF, test$spam)
predictionTestRF = prediction(predTrainRF, test$spam)
predictionTestRF = prediction(predTrainRF, train$spam)
predictionTestRF = prediction(predTestRF, test$spam)
predTestRF = predict(spamRF, newdata=test)[,2]
predictionTestRF = prediction(predTestRF, test$spam)
predTestRF = predict(spamRF, newdata=test)
predTestRF = predict(spamRF, newdata=test, type="prob")[,2]
predictionTestRF = prediction(predTestRF, test$spam)
as.numeric(performance(predictionTestRF, "auc")@y.values) #0.9627517
predictionTestCART = prediction(predTestCART, test$spam)
predTestCART = predict(spamCART, newdata=test, type="class")[,2]
predTestCART = predict(spamCART, newdata=test, type="class")[,1]
predTestCART = predict(spamCART, newdata=test, type="class")
predictionTestCART = prediction(predTestCART, test$spam)
predTestCART
predTestRF
predTestCART[,2]
predTestCART[,1]
predTestCART[2,]
predTestCART[[1]]
predTestCART
predTestCART[,2]
predTestCART = predict(spamCART, newdata=test)[,2]
predictionTestCART = prediction(predTestCART, test$spam)
as.numeric(performance(predictionTestCART, "auc")@y.values) #0.9627517
